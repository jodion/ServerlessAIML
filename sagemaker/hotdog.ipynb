{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hotdog vs not hotdog with image-classification and transfer learning\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Understanding Transfer Learning](#Understanding-Transfer-Learning)\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Prepare the data](#Prepare-the-data)\n",
    "1. [Training the new Image Classification Model](#Training-the-new-Image-Classification-Model)\n",
    "1. [Deployment of the model](#Deployment-of-the-model)\n",
    "1. [Inference of test data](#Inference-of-test-data)\n",
    "1. [Optimize the model based on the architecture](#Optimize-the-model-based-on-the-architecture)\n",
    "1. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "This notebook will create a model to detect hotdogs. A SageMaker GroundTruth labeling job should have been run to create an Augmented Manifest file from a small subset of images. Unfortunately, that's not enough for training the model. Instead, the caltech256 dataset will be used to supplement this data. Once the data will be merged in the form of an Augmented Manifest, training will begin. Instead of starting from scratch which would require many more images to train on, the SageMaker built-in image-classification algorithm will be used with transfer learning to create the model. The model will then be deployed and tested against another small dataset. Finally, the model will be compiled using SageMaker Neo for a Raspberry Pi and a Jetson Nano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Understanding Transfer Learning\n",
    "Before we move on, let’s look at Resnet, a NN topology that can achieve a high accuracy in image classification. It won the 2015 ImageNet Large Scale Visual Recognition Challenge for best object classifier, and it’s one of the most commonly used NNs for computer vision problems.\n",
    "\n",
    "Topologies like Resnet are called a convolutional neural network (CNN) because the network’s input layers execute convolution operations on the input image. A convolution is a mathematical function that emulates the visual cortex of an animal. A CNN has several convolution layers that learn image filters. These filters extract features from the input images such as edges, parts, and bodies. These features are then routed through the hidden or inner layers to the output layer. In the context of image classification, the output layer has one output per category.\n",
    "\n",
    "\n",
    "\n",
    "Consider a trained neural network capable of classifying a dog, as shown in the following image. \n",
    "\n",
    "![neuralnetexample](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/05/04/ImagesSageMaker5.png) \n",
    "\n",
    "The convolution layers extract some features from the dog image, and the rest of the layers route these features to the correct output of the last layer with a high confidence.\n",
    "\n",
    "Transfer learning is a technique used for reducing the time required for training a new model. Instead of training your model from scratch, you can use a modified pre-trained model and continue training it with your dataset. That’s why it’s called transfer learning: the knowledge learned by one NN is transferring to another NN.\n",
    "\n",
    "It means, for instance, that if you want a model that can classify cars brand/model/year and you already have a pre-trained model, it makes sense to change it a little bit and retrain it with your own dataset in order to create a new model that will solve this new problem.\n",
    "\n",
    "The Amazon SageMaker built-in algorithm for image classification is already prepared for transfer learning. You just need to set a given parameter to true, and your model will use this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Prerequisites\n",
    "This notebook assumes that you have ran a SageMaker GroundTruth labeling job and that it has completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the different variables to be used for training the model. \n",
    "* **role**: Will be the role ARN that SageMaker will use to create resources\n",
    "* **sess**: Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "* **bucket**: Contains the name of the bucket that SageMaker will use. It will create it if it doesn't already exist. However, this should already exist as it's the bucket that was used for the SageMaker GroundTruth labeling job.\n",
    "* **training_image**: The URI of the image-classification docker image stored in ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825641698319.dkr.ecr.us-east-2.amazonaws.com/image-classification:1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Get the role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Set the SageMaker Client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the default bucket, create it if it doesn't exist\n",
    "sess = sagemaker.Session()\n",
    "bucket=sess.default_bucket()\n",
    "\n",
    "# The prefix for everything in the bucket\n",
    "prefix=\"hotdog\"\n",
    "\n",
    "# Get the URL of the image-classification docker image from ECR\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification')\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare the data\n",
    "First, we will augment the SageMaker GroundTruth dataset with caltech256.\n",
    "\n",
    "The caltech 256 dataset consist of images from 257 categories (the last one being a clutter category) and has 30k images with a minimum of 80 images and a maximum of about 800 images per category. \n",
    "\n",
    "As we only care about hotdogs and not hotdogs, we will take one image from each category as well as the entire clutter category to form the \"nothotdog\" category. We will also use the \"108.hot-dog\" category to supplement our current images.\n",
    "\n",
    "The Amazon SageMaker Image Classification algorithm supports both RecordIO (application/x-recordio) and image (image/png, image/jpeg, and application/x-image) content types for training in file mode and supports RecordIO (application/x-recordio) content type for training in pipe mode. However you can also train in pipe mode using the image files (image/png, image/jpeg, and application/x-image), without creating RecordIO files, by using the augmented manifest format. \n",
    "\n",
    "With Pipe input mode, the dataset is streamed directly to the training instances instead of being downloaded first. This means that the training jobs start sooner, finish quicker, and need less disk space. Since the output of a SageMaker GroundTruth labeling job is an Augmented Manifest Image Format, we can directly use it for our training. Once downloaded, we will have to append the caltech 256 data to that augmented manifest.\n",
    "\n",
    "The output of a SageMaker GroundTruth labeling job is an Augmented Manifest Image Format. So we will use that file as the basis and append to it the rest of the images from caltech256.\n",
    "\n",
    "Then, we will split the data for training and validation into two augmented manifest files.\n",
    "\n",
    "Finally, we will upload the images used from caltech256 and the two augmented manifest files to S3 to be ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the caltech256 dataset and create the two categories\n",
    "Two categories need to be created. The _hotdog_ and _nothotdog_. We will create 2 folders and copy the appropriate files into them.\n",
    "* We will copy the _108.hot-dog_ category pictures into the _hotdog_ category folder.\n",
    "* We will copy the _257.clutter_ category pictures into the _nothotdog_ category folder.\n",
    "* We will copy one from from every category except the above two into the _nothotdog_ category folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jondion-public/256_ObjectCategories.tar to ./256_ObjectCategories.tar\n"
     ]
    }
   ],
   "source": [
    "#wget http://www.vision.caltech.edu/Image_Datasets/Caltech256/256_ObjectCategories.tar -q\n",
    "!aws s3 cp s3://jondion-public/256_ObjectCategories.tar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Remove old directories and create the new ones\n",
    "rm -rf caltech256\n",
    "rm -rf 256_ObjectCategories\n",
    "mkdir -p caltech256/hotdog caltech256/nothotdog\n",
    "\n",
    "# Extract images\n",
    "tar -xf 256_ObjectCategories.tar\n",
    "\n",
    "# Add the hotdogs to the hotdog dataset\n",
    "cp 256_ObjectCategories/108.hot-dog/* caltech256/hotdog/\n",
    "# Add all clutter to the nothotdog dataset\n",
    "cp 256_ObjectCategories/257.clutter/* caltech256/nothotdog/\n",
    "\n",
    "# Take one image from every category and move it to the nothotdog dataset\n",
    "for i in 256_ObjectCategories/*; do\n",
    "    for j in `ls $i/*.jpg | grep -v \"108.hot-dog\" | grep -v \"257.clutter\" | shuf | head -n 1`; do\n",
    "        cp $j caltech256/nothotdog/\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the datasets\n",
    "To merge the caltech256 dataset and the augmented output manifest from the SageMaker GroundTruth labeling job, we will first download the output manifest and load it as a JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-761424745283/hotdog/hotdog10/manifests/output/output.manifest to ./output.manifest\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/hotdog1.jpg\",\"hotdog\":1,\"hotdog-metadata\":{\"confidence\":0.55,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829893\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/hotdog2.jpg\",\"hotdog\":1,\"hotdog-metadata\":{\"confidence\":0.55,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829871\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/hotdog3.jpg\",\"hotdog\":1,\"hotdog-metadata\":{\"confidence\":0.57,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:36:51.015326\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other1.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.89,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829907\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other2.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.84,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:36:51.015348\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other3.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.89,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829919\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other4.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.84,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:36:51.015362\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other5.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.89,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829930\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other6.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.89,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829952\",\"type\":\"groundtruth/image-classification\"}}\n",
      "{\"source-ref\":\"s3://sagemaker-us-east-2-761424745283/hotdog/dataset/other7.jpg\",\"hotdog\":0,\"hotdog-metadata\":{\"confidence\":0.89,\"job-name\":\"labeling-job/hotdog10\",\"class-name\":\"Not Hotdog\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-11-06T00:37:57.829941\",\"type\":\"groundtruth/image-classification\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "groundTruthOutputManifest = sagemaker_client.list_labeling_jobs(MaxResults=1)['LabelingJobSummaryList'][0]['LabelingJobOutput']['OutputDatasetS3Uri']\n",
    "\n",
    "!aws s3 cp $groundTruthOutputManifest output.manifest\n",
    "!head output.manifest\n",
    "\n",
    "# Import the file into output\n",
    "with open('output.manifest', 'r') as f:\n",
    "    manifest = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are many fields that SageMaker GroundTruth automatically created. However, only the the _source-ref_ and _hotdog_ attributes are required for the training job. That's why we will only need to add those 2 attributes based on the caltech256 data.\n",
    "* **source-ref**: The S3 location of the image\n",
    "* **hotdog**: `0` means that there is no hotdog while `1` means that a hotdog was found\n",
    "\n",
    "We will loop through each files in the category folder (hotdog and nothotdog) and append an entry in the augmented _manifest_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hotdogdir = \"caltech256/hotdog\"\n",
    "for fname in sorted(os.listdir(hotdogdir)):\n",
    "    fpath = os.path.join(hotdogdir, fname)\n",
    "    manifest.append({'source-ref': 's3://{}/{}/{}'.format(bucket, prefix, fpath),'hotdog': 1})\n",
    "\n",
    "nothotdogdir = \"caltech256/nothotdog\"\n",
    "for fname in sorted(os.listdir(nothotdogdir)):\n",
    "    fpath = os.path.join(nothotdogdir, fname)\n",
    "    manifest.append({'source-ref': 's3://{}/{}/{}'.format(bucket, prefix, fpath),'hotdog': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and validation\n",
    "First, we will randomize the data and then split the data between training and validate at 80/20. The last step is to create the _train.manifest_ and _validation.manifest_ files with the appropriate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use numpy to shuffle the data\n",
    "np.random.shuffle(manifest)\n",
    "\n",
    "# Split dataset to 80/20 ration for training and validation\n",
    "dataset_size = len(manifest)\n",
    "train_test_split_index = round(dataset_size*0.8)\n",
    "train_data = manifest[:train_test_split_index]\n",
    "validation_data = manifest[train_test_split_index:]\n",
    "\n",
    "#Create training and validation files\n",
    "num_training_samples = 0\n",
    "with open('train.manifest', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        num_training_samples += 1\n",
    "    \n",
    "with open('validation.manifest', 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3\n",
    "The last step is to upload the images used from caltech256 and both manifest files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.manifest to s3://sagemaker-us-east-2-761424745283/hotdog/train.manifest\n",
      "upload: ./validation.manifest to s3://sagemaker-us-east-2-761424745283/hotdog/validation.manifest\n"
     ]
    }
   ],
   "source": [
    "# Upload the pictures to S3\n",
    "s3train = 's3://{}/{}/caltech256/'.format(bucket, prefix)\n",
    "!aws s3 cp caltech256 $s3train --recursive --quiet\n",
    "\n",
    "# Upload the augmented manifest files to S3\n",
    "s3train = 's3://{}/{}/train.manifest'.format(bucket, prefix)\n",
    "!aws s3 cp train.manifest $s3train\n",
    "s3validation = 's3://{}/{}/validation.manifest'.format(bucket, prefix)\n",
    "!aws s3 cp validation.manifest $s3validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the new Image Classification Model\n",
    "Now that we are done with all the setup that is needed, we are ready to train our image classification. To begin, we will create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job.\n",
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "* **training_image**: The URI to the docker image in ECR containing the algorithm.\n",
    "* **role**: The role that the instances will use to training data in S3.\n",
    "* **train_instance_count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. In Pipe mode, distributed training isn't supported.\n",
    "* **train_instance_type**: This indicates the type of machine on which to run the training. For image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes.\n",
    "* **train_volume_size**: The size of the EBS volume attached to the training instance.\n",
    "* **train_max_run**: Timeout in seconds for training. After this amount of time Amazon SageMaker terminates the job regardless of its current status.\n",
    "* **input_mode**: The input mode that the algorithm supports.  Valid modes: _File_ - Amazon SageMaker copies the training dataset from the S3 location to a local directory. _Pipe_ - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe. We will use Pipe here.\n",
    "* **output_path**: This the s3 folder in which the training output is stored.\n",
    "* **sagemaker_session**: Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                     role, \n",
    "                                     train_instance_count=1, \n",
    "                                     train_instance_type='ml.p3.2xlarge',\n",
    "                                     train_volume_size = 50,\n",
    "                                     train_max_run = 86400,\n",
    "                                     input_mode= 'Pipe',\n",
    "                                     output_path=s3_output_location,\n",
    "                                     sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the above set of parameters, there are hyperparameters that are specific to the algorithm. These are:\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. We use 18 in this samples but other values such as 50, 152 can be used.\n",
    "* **use_pretrained_model**: Set to 1 to use pretrained model for transfer learning.\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the new dataset. Imagenet was trained with 1000 output classes but the number of output classes can be changed for fine-tuning. We only have 2 for our example.\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 15240 for caltech dataset with the current split.\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.\n",
    "* **epochs**: Number of training epochs.\n",
    "* **learning_rate**: Learning rate for training.\n",
    "* **top_k**: Report the top-k accuracy during training.\n",
    "* **resize**: Resize the image before using it for training. The images are resized so that the shortest side is of this parameter. If the parameter is not set, then the training data is used as such without resizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.set_hyperparameters(num_layers = 18,\n",
    "                         use_pretrained_model = 1,\n",
    "                         image_shape = \"3,224,224\",\n",
    "                         num_classes = 2,\n",
    "                         num_training_samples = num_training_samples,\n",
    "                         mini_batch_size = 32,\n",
    "                         epochs = 30,\n",
    "                         learning_rate = 0.001,\n",
    "                         top_k = 2,\n",
    "                         resize = 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel configuration\n",
    "We then create a channel configuration for S3 data sources that provide additional information as well as the path to the training dataset using a `sagemaker.session.s3_input`.\n",
    "* **s3_data**: The S3 key to the augmented manifest file.\n",
    "* **content_type**: MIME type of the input data.\n",
    "* **s3_data_type**: When set to 'AugmentedManifestFile', then ``s3_data`` defines a single S3 augmented manifest file listing the S3 data to train on.\n",
    "* **attribute_names**: A list of one or more attribute names to use that are found in a specified AugmentedManifestFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_data=s3train, \n",
    "                                        content_type='application/x-recordio', \n",
    "                                        record_wrapping='RecordIO',\n",
    "                                        s3_data_type='AugmentedManifestFile', \n",
    "                                        attribute_names=[\"source-ref\",\"hotdog\"])\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3_data=s3validation, \n",
    "                                             content_type='application/x-recordio', \n",
    "                                             record_wrapping='RecordIO',\n",
    "                                             s3_data_type='AugmentedManifestFile', \n",
    "                                             attribute_names=[\"source-ref\",\"hotdog\"])\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "Start training by calling the fit method from the estimator passing the information about the training data via the _inputs_ argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 00:42:29 Starting - Starting the training job...\n",
      "2019-11-06 00:42:30 Starting - Launching requested ML instances...\n",
      "2019-11-06 00:43:24 Starting - Preparing the instances for training......\n",
      "2019-11-06 00:44:20 Downloading - Downloading input data...\n",
      "2019-11-06 00:44:37 Training - Downloading the training image....\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'top_k': u'2', u'use_pretrained_model': u'1', u'epochs': u'30', u'num_training_samples': u'942', u'num_layers': u'18', u'num_classes': u'2', u'mini_batch_size': u'32', u'image_shape': u'3,224,224', u'resize': u'224'}\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] Final configuration: {u'top_k': u'2', u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': u'30', u'resize': u'224', u'lr_scheduler_factor': 0.1, u'num_layers': u'18', u'precision_dtype': u'float32', u'mini_batch_size': u'32', u'num_classes': u'2', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'942'}\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] multi_label: 0\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] ---- Parameters ----\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] num_layers: 18\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] epochs: 30\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] image resize size: 224\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] optimizer: sgd\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] momentum: 0.9\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] learning_rate: 0.001\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] num_training_samples: 942\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] mini_batch_size: 32\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] num_classes: 2\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] augmentation_type: None\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] kv_store: device\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] top_k: 2\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] --------------------\u001b[0m\n",
      "\u001b[31m[00:45:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[00:45:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:35 INFO 140445251127104] Setting number of threads: 7\u001b[0m\n",
      "\u001b[31m[00:45:43] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:45 INFO 140445251127104] Epoch[0] Batch [20]#011Speed: 280.534 samples/sec#011accuracy=0.822917#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:45 INFO 140445251127104] Epoch[0] Train-accuracy=0.855603\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:45 INFO 140445251127104] Epoch[0] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:45 INFO 140445251127104] Epoch[0] Time cost=2.676\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:46 INFO 140445251127104] Epoch[0] Validation-accuracy=0.972656\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:46 INFO 140445251127104] Storing the best model with validation accuracy: 0.972656\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:46 INFO 140445251127104] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:48 INFO 140445251127104] Epoch[1] Batch [20]#011Speed: 365.392 samples/sec#011accuracy=0.974702#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\n",
      "2019-11-06 00:45:32 Training - Training image download completed. Training in progress.\u001b[31m[11/06/2019 00:45:48 INFO 140445251127104] Epoch[1] Train-accuracy=0.978448\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:48 INFO 140445251127104] Epoch[1] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:48 INFO 140445251127104] Epoch[1] Time cost=2.167\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:48 INFO 140445251127104] Epoch[1] Validation-accuracy=0.972656\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:50 INFO 140445251127104] Epoch[2] Batch [20]#011Speed: 369.927 samples/sec#011accuracy=0.986607#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Epoch[2] Train-accuracy=0.989224\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Epoch[2] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Epoch[2] Time cost=2.123\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Epoch[2] Validation-accuracy=0.976562\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Storing the best model with validation accuracy: 0.976562\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:51 INFO 140445251127104] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:53 INFO 140445251127104] Epoch[3] Batch [20]#011Speed: 319.384 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:54 INFO 140445251127104] Epoch[3] Train-accuracy=0.998922\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:54 INFO 140445251127104] Epoch[3] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:54 INFO 140445251127104] Epoch[3] Time cost=2.395\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:54 INFO 140445251127104] Epoch[3] Validation-accuracy=0.976562\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:56 INFO 140445251127104] Epoch[4] Batch [20]#011Speed: 365.223 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:57 INFO 140445251127104] Epoch[4] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:57 INFO 140445251127104] Epoch[4] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:57 INFO 140445251127104] Epoch[4] Time cost=2.167\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:57 INFO 140445251127104] Epoch[4] Validation-accuracy=0.976562\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:59 INFO 140445251127104] Epoch[5] Batch [20]#011Speed: 397.456 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:59 INFO 140445251127104] Epoch[5] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:59 INFO 140445251127104] Epoch[5] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:45:59 INFO 140445251127104] Epoch[5] Time cost=2.040\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:00 INFO 140445251127104] Epoch[5] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:00 INFO 140445251127104] Storing the best model with validation accuracy: 0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:00 INFO 140445251127104] Saved checkpoint to \"/opt/ml/model/image-classification-0006.params\"\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:01 INFO 140445251127104] Epoch[6] Batch [20]#011Speed: 416.765 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:02 INFO 140445251127104] Epoch[6] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:02 INFO 140445251127104] Epoch[6] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:02 INFO 140445251127104] Epoch[6] Time cost=1.929\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:02 INFO 140445251127104] Epoch[6] Validation-accuracy=0.980469\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[11/06/2019 00:46:05 INFO 140445251127104] Epoch[7] Batch [20]#011Speed: 230.961 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:05 INFO 140445251127104] Epoch[7] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:05 INFO 140445251127104] Epoch[7] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:05 INFO 140445251127104] Epoch[7] Time cost=3.160\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:06 INFO 140445251127104] Epoch[7] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:09 INFO 140445251127104] Epoch[8] Batch [20]#011Speed: 199.893 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:10 INFO 140445251127104] Epoch[8] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:10 INFO 140445251127104] Epoch[8] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:10 INFO 140445251127104] Epoch[8] Time cost=3.594\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:10 INFO 140445251127104] Epoch[8] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:12 INFO 140445251127104] Epoch[9] Batch [20]#011Speed: 419.998 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:12 INFO 140445251127104] Epoch[9] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:12 INFO 140445251127104] Epoch[9] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:12 INFO 140445251127104] Epoch[9] Time cost=1.913\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:12 INFO 140445251127104] Epoch[9] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:14 INFO 140445251127104] Epoch[10] Batch [20]#011Speed: 374.285 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:15 INFO 140445251127104] Epoch[10] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:15 INFO 140445251127104] Epoch[10] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:15 INFO 140445251127104] Epoch[10] Time cost=2.100\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:15 INFO 140445251127104] Epoch[10] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:17 INFO 140445251127104] Epoch[11] Batch [20]#011Speed: 398.896 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:17 INFO 140445251127104] Epoch[11] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:17 INFO 140445251127104] Epoch[11] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:17 INFO 140445251127104] Epoch[11] Time cost=1.991\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:18 INFO 140445251127104] Epoch[11] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:20 INFO 140445251127104] Epoch[12] Batch [20]#011Speed: 415.355 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:20 INFO 140445251127104] Epoch[12] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:20 INFO 140445251127104] Epoch[12] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:20 INFO 140445251127104] Epoch[12] Time cost=1.928\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:20 INFO 140445251127104] Epoch[12] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:22 INFO 140445251127104] Epoch[13] Batch [20]#011Speed: 440.709 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:22 INFO 140445251127104] Epoch[13] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:22 INFO 140445251127104] Epoch[13] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:22 INFO 140445251127104] Epoch[13] Time cost=1.844\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:23 INFO 140445251127104] Epoch[13] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:24 INFO 140445251127104] Epoch[14] Batch [20]#011Speed: 452.835 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:25 INFO 140445251127104] Epoch[14] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:25 INFO 140445251127104] Epoch[14] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:25 INFO 140445251127104] Epoch[14] Time cost=1.831\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:25 INFO 140445251127104] Epoch[14] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:27 INFO 140445251127104] Epoch[15] Batch [20]#011Speed: 418.477 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:27 INFO 140445251127104] Epoch[15] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:27 INFO 140445251127104] Epoch[15] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:27 INFO 140445251127104] Epoch[15] Time cost=1.917\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:28 INFO 140445251127104] Epoch[15] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:29 INFO 140445251127104] Epoch[16] Batch [20]#011Speed: 443.553 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:30 INFO 140445251127104] Epoch[16] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:30 INFO 140445251127104] Epoch[16] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:30 INFO 140445251127104] Epoch[16] Time cost=1.829\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:30 INFO 140445251127104] Epoch[16] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:32 INFO 140445251127104] Epoch[17] Batch [20]#011Speed: 411.682 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:32 INFO 140445251127104] Epoch[17] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:32 INFO 140445251127104] Epoch[17] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:32 INFO 140445251127104] Epoch[17] Time cost=1.942\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:33 INFO 140445251127104] Epoch[17] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:35 INFO 140445251127104] Epoch[18] Batch [20]#011Speed: 342.914 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:35 INFO 140445251127104] Epoch[18] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:35 INFO 140445251127104] Epoch[18] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:35 INFO 140445251127104] Epoch[18] Time cost=2.256\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:35 INFO 140445251127104] Epoch[18] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:37 INFO 140445251127104] Epoch[19] Batch [20]#011Speed: 431.976 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:38 INFO 140445251127104] Epoch[19] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:38 INFO 140445251127104] Epoch[19] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:38 INFO 140445251127104] Epoch[19] Time cost=1.875\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:38 INFO 140445251127104] Epoch[19] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:40 INFO 140445251127104] Epoch[20] Batch [20]#011Speed: 293.033 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:41 INFO 140445251127104] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:41 INFO 140445251127104] Epoch[20] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:41 INFO 140445251127104] Epoch[20] Time cost=2.571\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:41 INFO 140445251127104] Epoch[20] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:43 INFO 140445251127104] Epoch[21] Batch [20]#011Speed: 400.209 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:43 INFO 140445251127104] Epoch[21] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:43 INFO 140445251127104] Epoch[21] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:43 INFO 140445251127104] Epoch[21] Time cost=2.006\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:44 INFO 140445251127104] Epoch[21] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:46 INFO 140445251127104] Epoch[22] Batch [20]#011Speed: 399.914 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:46 INFO 140445251127104] Epoch[22] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:46 INFO 140445251127104] Epoch[22] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:46 INFO 140445251127104] Epoch[22] Time cost=1.991\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:46 INFO 140445251127104] Epoch[22] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:48 INFO 140445251127104] Epoch[23] Batch [20]#011Speed: 426.107 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:49 INFO 140445251127104] Epoch[23] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:49 INFO 140445251127104] Epoch[23] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:49 INFO 140445251127104] Epoch[23] Time cost=1.900\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:49 INFO 140445251127104] Epoch[23] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:51 INFO 140445251127104] Epoch[24] Batch [20]#011Speed: 362.546 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:51 INFO 140445251127104] Epoch[24] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:51 INFO 140445251127104] Epoch[24] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:51 INFO 140445251127104] Epoch[24] Time cost=2.153\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:52 INFO 140445251127104] Epoch[24] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:54 INFO 140445251127104] Epoch[25] Batch [20]#011Speed: 311.477 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:54 INFO 140445251127104] Epoch[25] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:54 INFO 140445251127104] Epoch[25] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:54 INFO 140445251127104] Epoch[25] Time cost=2.439\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:55 INFO 140445251127104] Epoch[25] Validation-accuracy=0.980469\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[11/06/2019 00:46:57 INFO 140445251127104] Epoch[26] Batch [20]#011Speed: 415.950 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:57 INFO 140445251127104] Epoch[26] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:57 INFO 140445251127104] Epoch[26] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:57 INFO 140445251127104] Epoch[26] Time cost=1.919\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:57 INFO 140445251127104] Epoch[26] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:46:59 INFO 140445251127104] Epoch[27] Batch [20]#011Speed: 400.796 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:00 INFO 140445251127104] Epoch[27] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:00 INFO 140445251127104] Epoch[27] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:00 INFO 140445251127104] Epoch[27] Time cost=1.987\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:00 INFO 140445251127104] Epoch[27] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:02 INFO 140445251127104] Epoch[28] Batch [20]#011Speed: 389.164 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:02 INFO 140445251127104] Epoch[28] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:02 INFO 140445251127104] Epoch[28] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:02 INFO 140445251127104] Epoch[28] Time cost=2.034\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:03 INFO 140445251127104] Epoch[28] Validation-accuracy=0.980469\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:04 INFO 140445251127104] Epoch[29] Batch [20]#011Speed: 456.459 samples/sec#011accuracy=1.000000#011top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:05 INFO 140445251127104] Epoch[29] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:05 INFO 140445251127104] Epoch[29] Train-top_k_accuracy_2=1.000000\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:05 INFO 140445251127104] Epoch[29] Time cost=1.796\u001b[0m\n",
      "\u001b[31m[11/06/2019 00:47:05 INFO 140445251127104] Epoch[29] Validation-accuracy=0.980469\u001b[0m\n",
      "\n",
      "2019-11-06 00:47:19 Uploading - Uploading generated training model\n",
      "2019-11-06 00:47:19 Completed - Training job completed\n",
      "Training seconds: 179\n",
      "Billable seconds: 179\n"
     ]
    }
   ],
   "source": [
    "ic.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment of the model\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. We will deploy the created model by using the deploy method in the estimator. This will create an _Endpoint Configuration_ and an _Endpoint_.\n",
    "* **endpoint_name**: Name to use for creating an Amazon SageMaker endpoint.\n",
    "* **initial_instance_count**: Minimum number of EC2 instances to deploy to an endpoint for prediction.\n",
    "* **instance_type**: Type of EC2 instance to deploy to an endpoint for prediction. We don't need a GPU in this case as we won't be constantly doing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ic_classifier = ic.deploy(endpoint_name='hotdog',\n",
    "                          initial_instance_count = 1,\n",
    "                          instance_type = 'ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inference of test data\n",
    "The algorithm supports image/png, image/jpeg, and application/x-image for inference sent as a bytearray.\n",
    "\n",
    "First, we will get 10 test images and store them in the _testdata_ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jondion-public/hotdog_testdata/hotdog3.jpg to testdata/hotdog3.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/hotdog5.jpg to testdata/hotdog5.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/hotdog1.jpg to testdata/hotdog1.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/nothotdog2.jpg to testdata/nothotdog2.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/nothotdog1.jpg to testdata/nothotdog1.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/nothotdog5.jpg to testdata/nothotdog5.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/nothotdog3.jpg to testdata/nothotdog3.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/hotdog4.jpg to testdata/hotdog4.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/hotdog2.jpg to testdata/hotdog2.jpg\n",
      "download: s3://jondion-public/hotdog_testdata/nothotdog4.jpg to testdata/nothotdog4.jpg\n"
     ]
    }
   ],
   "source": [
    "!mkdir testdata\n",
    "!aws s3 cp s3://jondion-public/hotdog_testdata/ testdata/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will iterate over every files from the _testdata_ folder, convert it into a bytearray and use the `sagemaker.RealTimePredictor` _ic_classifier_ object to execute a prediction. Even though it looks like a function call, like many of the other function calls done to the SageMaker API, this is an API call using HTTPS POST to the SageMaker Endpoint we created above.\n",
    "\n",
    "For each of the classes of our model (we only have 2: hotdog, nothotdog), it will return a probability. We will look at the highest probability of the two to determine if it's a hotdog or nothotdog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file hotdog1.jpg is a hotdog with a probability of 0.9970898032188416\n",
      "The file hotdog2.jpg is a hotdog with a probability of 0.9174467325210571\n",
      "The file hotdog3.jpg is a hotdog with a probability of 0.6538909077644348\n",
      "The file hotdog4.jpg is a hotdog with a probability of 0.9830251932144165\n",
      "The file hotdog5.jpg is a hotdog with a probability of 0.90871262550354\n",
      "The file nothotdog1.jpg is a nothotdog with a probability of 0.9836713075637817\n",
      "The file nothotdog2.jpg is a nothotdog with a probability of 0.996351957321167\n",
      "The file nothotdog3.jpg is a nothotdog with a probability of 0.9999521970748901\n",
      "The file nothotdog4.jpg is a nothotdog with a probability of 0.9969801306724548\n",
      "The file nothotdog5.jpg is a nothotdog with a probability of 0.9981411695480347\n"
     ]
    }
   ],
   "source": [
    "# Define our 2 categories and the folder with the test pictures\n",
    "object_categories = ['nothotdog', 'hotdog']\n",
    "test_image_folder = 'testdata'\n",
    "\n",
    "# Loop through all files\n",
    "for fname in sorted(os.listdir(test_image_folder)):\n",
    "    fpath = os.path.join(test_image_folder, fname)\n",
    "    \n",
    "    # Make sure it's a file\n",
    "    if os.path.isfile(fpath):\n",
    "        \n",
    "        # Convert the file into a bytearray\n",
    "        with open(fpath, 'rb') as f:\n",
    "            payload = f.read()\n",
    "            payload = bytearray(payload)\n",
    "\n",
    "        # Predict\n",
    "        ic_classifier.content_type = 'application/x-image'\n",
    "        result = json.loads(ic_classifier.predict(payload))\n",
    "\n",
    "        # The result will output the probabilities for all classes\n",
    "        # Find the class with maximum probability and print the class index as well as it's probability\n",
    "        index = np.argmax(result)\n",
    "        print(\"The file \" + fname + \" is a \" + object_categories[index] + \" with a probability of \" + str(result[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimize the model based on the architecture\n",
    "\n",
    "We will use SageMaker Neo to train a machine learning model so it can be run at the edge. We could also have done the optimization to run it for our particular type of instance. In this case, we will create two models. One optimized for an NVIDIA Jetson Nano and one for a Raspberri Pi 3 B+. Amazon SageMaker Neo optimizes models to run up to twice as fast, with less than a tenth of the memory footprint, with no loss in accuracy.\n",
    "\n",
    "We will use the `compile_model` function of the Estimator to create a new optimized model with the following parameters:\n",
    "* **target_instance_family**: Identifies the device that you want to run your model after compilation. We will use jetson_nano and rasp3b\n",
    "* **input_shape**: Specifies the name and shape of the expected inputs for your trained model in json dictionary form. [1, 3, 224, 224] means a single (1) RGB (3) image of size 224x224.\n",
    "* **output_path**: Specifies the S3 location to store the compiled model.\n",
    "* **framework**: The framework that is used to train the original model. The image-classification algorithm was trained under MXNET.\n",
    "* **framework_version**: The version of the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA Jetson Nano\n",
    "You can ignore the warning in red which says that we can't deploy this model on SageMaker. This will be done via AWS IoT GreenGrass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jetson_model = ic.compile_model(target_instance_family = 'jetson_nano',\n",
    "#                                  input_shape = {'data':[1, 3, 224, 224]},\n",
    "#                                  output_path = 's3://{}/{}/neo_output/'.format(bucket, prefix),\n",
    "#                                  framework = 'mxnet',\n",
    "#                                  framework_version = '1.2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raspberry Pi 3 B+\n",
    "You can ignore the warning in red which says that we can't deploy this model on SageMaker. This will be done via AWS IoT GreenGrass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasp3b_model = ic.compile_model(target_instance_family = 'rasp3b',\n",
    "#                                  input_shape = {'data':[1, 3, 224, 224]},\n",
    "#                                  output_path = 's3://{}/{}/neo_output/'.format(bucket, prefix),\n",
    "#                                  framework = 'mxnet',\n",
    "#                                  framework_version = '1.2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model created by the compilation includes the file _model-shapes.json_ which isn't used by the code used on the Raspberri Pi. In fact, it creates an issue when that file is included in the model. This should be fixed in a new version of the Deep Learning Runtime. \n",
    "\n",
    "To fix the issue, we will download the model, re-archive the model and upload it back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3rasp3b = 's3://{}/{}/neo_output/model-rasp3b.tar.gz'.format(bucket, prefix)\n",
    "# !aws s3 cp $s3rasp3b .\n",
    "# !tar -zxf model-rasp3b.tar.gz --exclude model-shapes.json\n",
    "# !tar -czf model-rasp3b.tar.gz compiled.params compiled_model.json compiled.so\n",
    "# !aws s3 cp model-rasp3b.tar.gz $s3rasp3b\n",
    "# !tar -ztf model-rasp3b.tar.gz\n",
    "# !rm compiled.params compiled_model.json compiled.so model-rasp3b.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean up\n",
    "\n",
    "When we're done with the endpoint, delete the Model and the Endpoint which will remove the Endpoint Configuration by default. The data will still be stored in S3 so make sure to remove it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove the data from the S3 bucket sagemaker-us-east-2-761424745283 under the prefix hotdog.\n"
     ]
    }
   ],
   "source": [
    "ic_classifier.delete_model()\n",
    "ic_classifier.delete_endpoint()\n",
    "print('Remove the data from the S3 bucket {} under the prefix {}.'.format(bucket, prefix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
